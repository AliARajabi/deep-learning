{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Art_Generation_with_Neural_Style_Transfer_v3a.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AliARajabi/deep-learning/blob/main/Art_Generation_with_Neural_Style_Transfer/Art_Generation_with_Neural_Style_Transfer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dU46OVKuf1Mu"
      },
      "source": [
        "import os\n",
        "import sys\n",
        "import scipy.io\n",
        "import scipy.misc\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.pyplot import imshow\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import pprint"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JNJaG9rwgUe0"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "class CONFIG:\n",
        "    IMAGE_WIDTH = 400\n",
        "    IMAGE_HEIGHT = 300\n",
        "    COLOR_CHANNELS = 3\n",
        "    NOISE_RATIO = 0.6\n",
        "    MEANS = np.array([123.68, 116.779, 103.939]).reshape((1,1,1,3)) \n",
        "    VGG_MODEL = 'pretrained-model/imagenet-vgg-verydeep-19.mat' # Pick the VGG 19-layer model by from the paper \"Very Deep Convolutional Networks for Large-Scale Image Recognition\".\n",
        "    STYLE_IMAGE = 'images/stone_style.jpg' # Style image to use.\n",
        "    CONTENT_IMAGE = 'images/content300.jpg' # Content image to use.\n",
        "    OUTPUT_DIR = 'output/'\n",
        "    \n",
        "def load_vgg_model(path):\n",
        "    \"\"\"\n",
        "    Returns a model for the purpose of 'painting' the picture.\n",
        "    Takes only the convolution layer weights and wrap using the TensorFlow\n",
        "    Conv2d, Relu and AveragePooling layer. VGG actually uses maxpool but\n",
        "    the paper indicates that using AveragePooling yields better results.\n",
        "    The last few fully connected layers are not used.\n",
        "    Here is the detailed configuration of the VGG model:\n",
        "        0 is conv1_1 (3, 3, 3, 64)\n",
        "        1 is relu\n",
        "        2 is conv1_2 (3, 3, 64, 64)\n",
        "        3 is relu    \n",
        "        4 is maxpool\n",
        "        5 is conv2_1 (3, 3, 64, 128)\n",
        "        6 is relu\n",
        "        7 is conv2_2 (3, 3, 128, 128)\n",
        "        8 is relu\n",
        "        9 is maxpool\n",
        "        10 is conv3_1 (3, 3, 128, 256)\n",
        "        11 is relu\n",
        "        12 is conv3_2 (3, 3, 256, 256)\n",
        "        13 is relu\n",
        "        14 is conv3_3 (3, 3, 256, 256)\n",
        "        15 is relu\n",
        "        16 is conv3_4 (3, 3, 256, 256)\n",
        "        17 is relu\n",
        "        18 is maxpool\n",
        "        19 is conv4_1 (3, 3, 256, 512)\n",
        "        20 is relu\n",
        "        21 is conv4_2 (3, 3, 512, 512)\n",
        "        22 is relu\n",
        "        23 is conv4_3 (3, 3, 512, 512)\n",
        "        24 is relu\n",
        "        25 is conv4_4 (3, 3, 512, 512)\n",
        "        26 is relu\n",
        "        27 is maxpool\n",
        "        28 is conv5_1 (3, 3, 512, 512)\n",
        "        29 is relu\n",
        "        30 is conv5_2 (3, 3, 512, 512)\n",
        "        31 is relu\n",
        "        32 is conv5_3 (3, 3, 512, 512)\n",
        "        33 is relu\n",
        "        34 is conv5_4 (3, 3, 512, 512)\n",
        "        35 is relu\n",
        "        36 is maxpool\n",
        "        37 is fullyconnected (7, 7, 512, 4096)\n",
        "        38 is relu\n",
        "        39 is fullyconnected (1, 1, 4096, 4096)\n",
        "        40 is relu\n",
        "        41 is fullyconnected (1, 1, 4096, 1000)\n",
        "        42 is softmax\n",
        "    \"\"\"\n",
        "    \n",
        "    vgg = scipy.io.loadmat(path)\n",
        "\n",
        "    vgg_layers = vgg['layers']\n",
        "    \n",
        "    def _weights(layer, expected_layer_name):\n",
        "        \"\"\"\n",
        "        Return the weights and bias from the VGG model for a given layer.\n",
        "        \"\"\"\n",
        "        wb = vgg_layers[0][layer][0][0][2]\n",
        "        W = wb[0][0]\n",
        "        b = wb[0][1]\n",
        "        layer_name = vgg_layers[0][layer][0][0][0][0]\n",
        "        assert layer_name == expected_layer_name\n",
        "        return W, b\n",
        "\n",
        "        return W, b\n",
        "\n",
        "    def _relu(conv2d_layer):\n",
        "        \"\"\"\n",
        "        Return the RELU function wrapped over a TensorFlow layer. Expects a\n",
        "        Conv2d layer input.\n",
        "        \"\"\"\n",
        "        return tf.nn.relu(conv2d_layer)\n",
        "\n",
        "    def _conv2d(prev_layer, layer, layer_name):\n",
        "        \"\"\"\n",
        "        Return the Conv2D layer using the weights, biases from the VGG\n",
        "        model at 'layer'.\n",
        "        \"\"\"\n",
        "        W, b = _weights(layer, layer_name)\n",
        "        W = tf.constant(W)\n",
        "        b = tf.constant(np.reshape(b, (b.size)))\n",
        "        return tf.nn.conv2d(prev_layer, filter=W, strides=[1, 1, 1, 1], padding='SAME') + b\n",
        "\n",
        "    def _conv2d_relu(prev_layer, layer, layer_name):\n",
        "        \"\"\"\n",
        "        Return the Conv2D + RELU layer using the weights, biases from the VGG\n",
        "        model at 'layer'.\n",
        "        \"\"\"\n",
        "        return _relu(_conv2d(prev_layer, layer, layer_name))\n",
        "\n",
        "    def _avgpool(prev_layer):\n",
        "        \"\"\"\n",
        "        Return the AveragePooling layer.\n",
        "        \"\"\"\n",
        "        return tf.nn.avg_pool(prev_layer, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
        "\n",
        "    # Constructs the graph model.\n",
        "    graph = {}\n",
        "    graph['input']   = tf.Variable(np.zeros((1, CONFIG.IMAGE_HEIGHT, CONFIG.IMAGE_WIDTH, CONFIG.COLOR_CHANNELS)), dtype = 'float32')\n",
        "    graph['conv1_1']  = _conv2d_relu(graph['input'], 0, 'conv1_1')\n",
        "    graph['conv1_2']  = _conv2d_relu(graph['conv1_1'], 2, 'conv1_2')\n",
        "    graph['avgpool1'] = _avgpool(graph['conv1_2'])\n",
        "    graph['conv2_1']  = _conv2d_relu(graph['avgpool1'], 5, 'conv2_1')\n",
        "    graph['conv2_2']  = _conv2d_relu(graph['conv2_1'], 7, 'conv2_2')\n",
        "    graph['avgpool2'] = _avgpool(graph['conv2_2'])\n",
        "    graph['conv3_1']  = _conv2d_relu(graph['avgpool2'], 10, 'conv3_1')\n",
        "    graph['conv3_2']  = _conv2d_relu(graph['conv3_1'], 12, 'conv3_2')\n",
        "    graph['conv3_3']  = _conv2d_relu(graph['conv3_2'], 14, 'conv3_3')\n",
        "    graph['conv3_4']  = _conv2d_relu(graph['conv3_3'], 16, 'conv3_4')\n",
        "    graph['avgpool3'] = _avgpool(graph['conv3_4'])\n",
        "    graph['conv4_1']  = _conv2d_relu(graph['avgpool3'], 19, 'conv4_1')\n",
        "    graph['conv4_2']  = _conv2d_relu(graph['conv4_1'], 21, 'conv4_2')\n",
        "    graph['conv4_3']  = _conv2d_relu(graph['conv4_2'], 23, 'conv4_3')\n",
        "    graph['conv4_4']  = _conv2d_relu(graph['conv4_3'], 25, 'conv4_4')\n",
        "    graph['avgpool4'] = _avgpool(graph['conv4_4'])\n",
        "    graph['conv5_1']  = _conv2d_relu(graph['avgpool4'], 28, 'conv5_1')\n",
        "    graph['conv5_2']  = _conv2d_relu(graph['conv5_1'], 30, 'conv5_2')\n",
        "    graph['conv5_3']  = _conv2d_relu(graph['conv5_2'], 32, 'conv5_3')\n",
        "    graph['conv5_4']  = _conv2d_relu(graph['conv5_3'], 34, 'conv5_4')\n",
        "    graph['avgpool5'] = _avgpool(graph['conv5_4'])\n",
        "    \n",
        "    return graph\n",
        "\n",
        "def generate_noise_image(content_image, noise_ratio = CONFIG.NOISE_RATIO):\n",
        "    \"\"\"\n",
        "    Generates a noisy image by adding random noise to the content_image\n",
        "    \"\"\"\n",
        "    \n",
        "    # Generate a random noise_image\n",
        "    noise_image = np.random.uniform(-20, 20, (1, CONFIG.IMAGE_HEIGHT, CONFIG.IMAGE_WIDTH, CONFIG.COLOR_CHANNELS)).astype('float32')\n",
        "    \n",
        "    # Set the input_image to be a weighted average of the content_image and a noise_image\n",
        "    input_image = noise_image * noise_ratio + content_image * (1 - noise_ratio)\n",
        "    \n",
        "    return input_image\n",
        "\n",
        "\n",
        "def reshape_and_normalize_image(image):\n",
        "    \"\"\"\n",
        "    Reshape and normalize the input image (content or style)\n",
        "    \"\"\"\n",
        "    \n",
        "    # Reshape image to mach expected input of VGG16\n",
        "    image = np.reshape(image, ((1,) + image.shape))\n",
        "    \n",
        "    # Substract the mean to match the expected input of VGG16\n",
        "    image = image - CONFIG.MEANS\n",
        "    \n",
        "    return image\n",
        "\n",
        "\n",
        "def save_image(path, image):\n",
        "    \n",
        "    # Un-normalize the image so that it looks good\n",
        "    image = image + CONFIG.MEANS\n",
        "    \n",
        "    # Clip and Save the image\n",
        "    image = np.clip(image[0], 0, 255).astype('uint8')\n",
        "    scipy.misc.imsave(path, image)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0WDSzxLhhAuz",
        "outputId": "d6da1cdb-a6cd-492f-e196-4434439efdd6"
      },
      "source": [
        "!git clone https://github.com/amanchadha/coursera-deep-learning-specialization.git"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'coursera-deep-learning-specialization'...\n",
            "remote: Enumerating objects: 1090, done.\u001b[K\n",
            "remote: Total 1090 (delta 0), reused 0 (delta 0), pack-reused 1090\u001b[K\n",
            "Receiving objects: 100% (1090/1090), 170.99 MiB | 27.74 MiB/s, done.\n",
            "Resolving deltas: 100% (56/56), done.\n",
            "Checking out files: 100% (943/943), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o5y5p3_ZjM_O",
        "outputId": "bb9a8fd9-37cf-4bf6-e947-07405e4cc0f7"
      },
      "source": [
        "!wget https://www.vlfeat.org/matconvnet/models/imagenet-vgg-verydeep-19.mat"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-04-01 22:55:03--  https://www.vlfeat.org/matconvnet/models/imagenet-vgg-verydeep-19.mat\n",
            "Resolving www.vlfeat.org (www.vlfeat.org)... 64.90.48.57\n",
            "Connecting to www.vlfeat.org (www.vlfeat.org)|64.90.48.57|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 534904783 (510M)\n",
            "Saving to: ‘imagenet-vgg-verydeep-19.mat’\n",
            "\n",
            "imagenet-vgg-veryde 100%[===================>] 510.12M  35.1MB/s    in 15s     \n",
            "\n",
            "2021-04-01 22:55:18 (34.3 MB/s) - ‘imagenet-vgg-verydeep-19.mat’ saved [534904783/534904783]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        },
        "id": "qhehw38JhJ1s",
        "outputId": "a44c5437-6b0e-427b-a2d3-defa86085545"
      },
      "source": [
        "pp = pprint.PrettyPrinter(indent=4)\n",
        "model = load_vgg_model(\"/content/imagenet-vgg-verydeep-19.mat\")\n",
        "pp.pprint(model)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-b5adc9b2f68a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mpp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpprint\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPrettyPrinter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_vgg_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/imagenet-vgg-verydeep-19.mat\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mpp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-966b7d20d129>\u001b[0m in \u001b[0;36mload_vgg_model\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0mgraph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0mgraph\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'input'\u001b[0m\u001b[0;34m]\u001b[0m   \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCONFIG\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIMAGE_HEIGHT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCONFIG\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIMAGE_WIDTH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCONFIG\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCOLOR_CHANNELS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'float32'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m     \u001b[0mgraph\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'conv1_1'\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0m_conv2d_relu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'input'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'conv1_1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m     \u001b[0mgraph\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'conv1_2'\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0m_conv2d_relu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'conv1_1'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'conv1_2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m     \u001b[0mgraph\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'avgpool1'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_avgpool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'conv1_2'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-966b7d20d129>\u001b[0m in \u001b[0;36m_conv2d_relu\u001b[0;34m(prev_layer, layer, layer_name)\u001b[0m\n\u001b[1;32m    105\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0mat\u001b[0m \u001b[0;34m'layer'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \"\"\"\n\u001b[0;32m--> 107\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_relu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_conv2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprev_layer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_avgpool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprev_layer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-966b7d20d129>\u001b[0m in \u001b[0;36m_conv2d\u001b[0;34m(prev_layer, layer, layer_name)\u001b[0m\n\u001b[1;32m     98\u001b[0m         \u001b[0mW\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprev_layer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrides\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'SAME'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_conv2d_relu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprev_layer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: conv2d_v2() got an unexpected keyword argument 'filter'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kkti-RxArUe6"
      },
      "source": [
        "content_image = scipy.misc.imread(\"images/louvre.jpg\")\n",
        "imshow(content_image);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QSA-HIjzrVMa"
      },
      "source": [
        "def compute_content_cost(a_C, a_G):\n",
        "    \"\"\"\n",
        "    Computes the content cost\n",
        "    \n",
        "    Arguments:\n",
        "    a_C -- tensor of dimension (1, n_H, n_W, n_C), hidden layer activations representing content of the image C \n",
        "    a_G -- tensor of dimension (1, n_H, n_W, n_C), hidden layer activations representing content of the image G\n",
        "    \n",
        "    Returns: \n",
        "    J_content -- scalar that you compute using equation 1 above.\n",
        "    \"\"\"\n",
        "    \n",
        "    ### START CODE HERE ###\n",
        "    # Retrieve dimensions from a_G (≈1 line)\n",
        "    m, n_H, n_W, n_C = a_G.shape\n",
        "    \n",
        "    new_shape = [int(m), int(n_H * n_W), int(n_C)]\n",
        "    \n",
        "    # Reshape a_C and a_G (≈2 lines)\n",
        "    a_C_unrolled = tf.reshape(a_C, new_shape)\n",
        "    a_G_unrolled = tf.reshape(a_G, new_shape)\n",
        "    \n",
        "    # compute the cost with tensorflow (≈1 line)\n",
        "    J_content = (.25 / float(int(n_H * n_W * n_C))) * tf.reduce_sum(np.power(a_G_unrolled - a_C_unrolled, 2))\n",
        "    ### END CODE HERE ###\n",
        "    \n",
        "    return J_content"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gc8IqiJMrZ0B"
      },
      "source": [
        "tf.set_random_seed(1)\n",
        "a_C = tf.random_normal([1, 4, 4, 3], mean=1, stddev=4)\n",
        "a_G = tf.random_normal([1, 4, 4, 3], mean=1, stddev=4)\n",
        "J_content = compute_content_cost(a_C, a_G)\n",
        "print(\"J_content = \" + str(J_content.eval()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gxNfyz3zrfpB"
      },
      "source": [
        "style_image = scipy.misc.imread(\"images/monet_800600.jpg\")\n",
        "imshow(style_image);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3WkV_JJ0rkGY"
      },
      "source": [
        "def gram_matrix(A):\n",
        "    \"\"\"\n",
        "    Argument:\n",
        "    A -- matrix of shape (n_C, n_H*n_W)\n",
        "    \n",
        "    Returns:\n",
        "    GA -- Gram matrix of A, of shape (n_C, n_C)\n",
        "    \"\"\"\n",
        "    \n",
        "    ### START CODE HERE ### (≈1 line)\n",
        "    GA = tf.matmul(A, tf.transpose(A))\n",
        "    ### END CODE HERE ###\n",
        "    \n",
        "    return GA"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tKevv8TErktS"
      },
      "source": [
        "tf.set_random_seed(1)\n",
        "A = tf.random_normal([3, 2*1], mean=1, stddev=4)\n",
        "GA = gram_matrix(A)\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VQR1LqM7rukS"
      },
      "source": [
        "def compute_layer_style_cost(a_S, a_G):\n",
        "    \"\"\"\n",
        "    Arguments:\n",
        "    a_S -- tensor of dimension (1, n_H, n_W, n_C), hidden layer activations representing style of the image S \n",
        "    a_G -- tensor of dimension (1, n_H, n_W, n_C), hidden layer activations representing style of the image G\n",
        "    \n",
        "    Returns: \n",
        "    J_style_layer -- tensor representing a scalar value, style cost defined above by equation (2)\n",
        "    \"\"\"\n",
        "    \n",
        "    ### START CODE HERE ###\n",
        "    # Retrieve dimensions from a_G (≈1 line)\n",
        "    m, n_H, n_W, n_C = a_G.get_shape().as_list()\n",
        "    \n",
        "    # Reshape the images to have them of shape (n_C, n_H*n_W) (≈2 lines)\n",
        "    a_S = tf.reshape(a_S, [n_H * n_W, n_C])\n",
        "    a_S = tf.transpose(a_S)\n",
        "    a_G = tf.reshape(a_G, [n_H * n_W, n_C])\n",
        "    a_G = tf.transpose(a_G)\n",
        "\n",
        "    # Computing gram_matrices for both images S and G (≈2 lines)\n",
        "    GS = gram_matrix(a_S)\n",
        "    GG = gram_matrix(a_G)\n",
        "\n",
        "    # Computing the loss (≈1 line)\n",
        "    factor = (.5 / (n_H * n_W * n_C)) ** 2\n",
        "    J_style_layer = factor * tf.reduce_sum(np.power(GS - GG, 2))\n",
        "    \n",
        "    ### END CODE HERE ###\n",
        "    \n",
        "    return J_style_layer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VZ_mKYLArwr5"
      },
      "source": [
        "tf.set_random_seed(1)\n",
        "a_S = tf.random_normal([1, 4, 4, 3], mean=1, stddev=4)\n",
        "a_G = tf.random_normal([1, 4, 4, 3], mean=1, stddev=4)\n",
        "J_style_layer = compute_layer_style_cost(a_S, a_G)\n",
        "    \n",
        "print(\"J_style_layer = \" + str(J_style_layer.eval()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Roj9-5tDryvB"
      },
      "source": [
        "STYLE_LAYERS = [\n",
        "    ('conv1_1', 0.2),\n",
        "    ('conv2_1', 0.2),\n",
        "    ('conv3_1', 0.2),\n",
        "    ('conv4_1', 0.2),\n",
        "    ('conv5_1', 0.2)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q0-PMM_9r2ko"
      },
      "source": [
        "def compute_style_cost(model, STYLE_LAYERS):\n",
        "    \"\"\"\n",
        "    Computes the overall style cost from several chosen layers\n",
        "    \n",
        "    Arguments:\n",
        "    model -- our tensorflow model\n",
        "    STYLE_LAYERS -- A python list containing:\n",
        "                        - the names of the layers we would like to extract style from\n",
        "                        - a coefficient for each of them\n",
        "    \n",
        "    Returns: \n",
        "    J_style -- tensor representing a scalar value, style cost defined above by equation (2)\n",
        "    \"\"\"\n",
        "    \n",
        "    # initialize the overall style cost\n",
        "    J_style = 0\n",
        "\n",
        "    for layer_name, coeff in STYLE_LAYERS:\n",
        "\n",
        "        # Select the output tensor of the currently selected layer\n",
        "        out = model[layer_name]\n",
        "\n",
        "        # Set a_S to be the hidden layer activation from the layer we have selected, by running the session on out\n",
        "        a_S = sess.run(out)\n",
        "\n",
        "        # Set a_G to be the hidden layer activation from same layer. Here, a_G references model[layer_name] \n",
        "        # and isn't evaluated yet. Later in the code, we'll assign the image G as the model input, so that\n",
        "        # when we run the session, this will be the activations drawn from the appropriate layer, with G as input.\n",
        "        a_G = out\n",
        "        \n",
        "        # Compute style_cost for the current layer\n",
        "        J_style_layer = compute_layer_style_cost(a_S, a_G)\n",
        "\n",
        "        # Add coeff * J_style_layer of this layer to overall style cost\n",
        "        J_style += coeff * J_style_layer\n",
        "\n",
        "    return J_style"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S-FLRK0gr3MZ"
      },
      "source": [
        "def total_cost(J_content, J_style, alpha = 10, beta = 40):\n",
        "    \"\"\"\n",
        "    Computes the total cost function\n",
        "    \n",
        "    Arguments:\n",
        "    J_content -- content cost coded above\n",
        "    J_style -- style cost coded above\n",
        "    alpha -- hyperparameter weighting the importance of the content cost\n",
        "    beta -- hyperparameter weighting the importance of the style cost\n",
        "    \n",
        "    Returns:\n",
        "    J -- total cost as defined by the formula above.\n",
        "    \"\"\"\n",
        "    \n",
        "    ### START CODE HERE ### (≈1 line)\n",
        "    J = alpha * J_content + beta * J_style\n",
        "    ### END CODE HERE ###\n",
        "    \n",
        "    return J"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AnIkYFfbr5uB"
      },
      "source": [
        "np.random.seed(3)\n",
        "J_content = np.random.randn()    \n",
        "J_style = np.random.randn()\n",
        "J = total_cost(J_content, J_style)\n",
        "print(\"J = \" + str(J))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WKyKovnYsBJQ"
      },
      "source": [
        "content_image = scipy.misc.imread(\"images/louvre_small.jpg\")\n",
        "content_image = reshape_and_normalize_image(content_image)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fa8vZgdPsBw0"
      },
      "source": [
        "style_image = scipy.misc.imread(\"images/monet.jpg\")\n",
        "style_image = reshape_and_normalize_image(style_image)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a02k0yPSsD8x"
      },
      "source": [
        "generated_image = generate_noise_image(content_image)\n",
        "imshow(generated_image[0]);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6dKTM81JsGGI"
      },
      "source": [
        "model = load_vgg_model(\"pretrained-model/imagenet-vgg-verydeep-19.mat\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CQ0Gs4iMsIHh"
      },
      "source": [
        "########################\n",
        "sess.run(model['input'].assign(content_image))\n",
        "\n",
        "# Select the output tensor of layer conv4_2\n",
        "out = model['conv4_2']\n",
        "\n",
        "# Set a_C to be the hidden layer activation from the layer we have selected\n",
        "a_C = sess.run(out)\n",
        "\n",
        "# Set a_G to be the hidden layer activation from same layer. Here, a_G references model['conv4_2'] \n",
        "# and isn't evaluated yet. Later in the code, we'll assign the image G as the model input, so that\n",
        "# when we run the session, this will be the activations drawn from the appropriate layer, with G as input.\n",
        "a_G = out\n",
        "\n",
        "# Compute the content cost\n",
        "J_content = compute_content_cost(a_C, a_G)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xWDNoeT2sPup"
      },
      "source": [
        "# Assign the input of the model to be the \"style\" image \n",
        "sess.run(model['input'].assign(style_image))\n",
        "\n",
        "# Compute the style cost\n",
        "J_style = compute_style_cost(model, STYLE_LAYERS)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wLhiWrcdsRjb"
      },
      "source": [
        "### START CODE HERE ### (1 line)\n",
        "J = total_cost(J_content, J_style, alpha = 10, beta = 40)\n",
        "### END CODE HERE ###"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OCYINWW2sTQ4"
      },
      "source": [
        "# define optimizer (1 line)\n",
        "optimizer = tf.train.AdamOptimizer(2.0)\n",
        "\n",
        "# define train_step (1 line)\n",
        "train_step = optimizer.minimize(J)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3-DiBWCssVER"
      },
      "source": [
        "def model_nn(sess, input_image, num_iterations = 200):\n",
        "    \n",
        "    # Initialize global variables (you need to run the session on the initializer)\n",
        "    ### START CODE HERE ### (1 line)\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    ### END CODE HERE ###\n",
        "    \n",
        "    # Run the noisy input image (initial generated image) through the model. Use assign().\n",
        "    ### START CODE HERE ### (1 line)\n",
        "    sess.run(model['input'].assign(input_image))\n",
        "    ### END CODE HERE ###\n",
        "    \n",
        "    for i in range(num_iterations):\n",
        "    \n",
        "        # Run the session on the train_step to minimize the total cost\n",
        "        ### START CODE HERE ### (1 line)\n",
        "        somthing = sess.run(train_step)\n",
        "        ### END CODE HERE ###\n",
        "        \n",
        "        # Compute the generated image by running the session on the current model['input']\n",
        "        ### START CODE HERE ### (1 line)\n",
        "        generated_image = sess.run(model['input'])\n",
        "        ### END CODE HERE ###\n",
        "\n",
        "        # Print every 20 iteration.\n",
        "        if i%20 == 0:\n",
        "            Jt, Jc, Js = sess.run([J, J_content, J_style])\n",
        "            print(\"Iteration \" + str(i) + \" :\")\n",
        "            print(\"total cost = \" + str(Jt))\n",
        "            print(\"content cost = \" + str(Jc))\n",
        "            print(\"style cost = \" + str(Js))\n",
        "            \n",
        "            # save current generated image in the \"/output\" directory\n",
        "            save_image(\"output/\" + str(i) + \".png\", generated_image)\n",
        "    \n",
        "    # save last generated image\n",
        "    save_image('output/generated_image.jpg', generated_image)\n",
        "    \n",
        "    return generated_image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bjXqCJkXsY0G"
      },
      "source": [
        "model_nn(sess, generated_image)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KWg4gTXosZSi"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}