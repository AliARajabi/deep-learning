{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyP4TJYjnlfQBpnkqy1wQfZb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AliARajabi/deep-learning/blob/main/RNN/text_Generation_Using_RNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "veYhJYvX2VPr"
      },
      "source": [
        "# Text generating using RNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5TRquitl2mFg"
      },
      "source": [
        "download text file \n",
        "\n",
        "It is shakespeare"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I9678LYXuiHS",
        "outputId": "38f97ad8-737f-43f4-8f32-381299ccc33e"
      },
      "source": [
        "!wget https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-04-15 16:33:59--  https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 74.125.20.128, 74.125.197.128, 74.125.135.128, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|74.125.20.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: ‘shakespeare.txt’\n",
            "\n",
            "\rshakespeare.txt       0%[                    ]       0  --.-KB/s               \rshakespeare.txt     100%[===================>]   1.06M  --.-KB/s    in 0.01s   \n",
            "\n",
            "2021-04-15 16:33:59 (104 MB/s) - ‘shakespeare.txt’ saved [1115394/1115394]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hEtmwxoM0coE"
      },
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from keras.preprocessing.text import text_to_word_sequence"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ifGxDP1M052d",
        "outputId": "0c610fb2-d72c-48f1-9fd8-6ed952be5867"
      },
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OPe9tcR02rGI"
      },
      "source": [
        "Read text file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "epWy3xjXwult"
      },
      "source": [
        "with open('/content/shakespeare.txt', \"r\") as f:\n",
        "  text=f.read().lower()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lrh_lcfhxRWq",
        "outputId": "fd54269d-ea73-4746-be2d-6c3839c867a1"
      },
      "source": [
        "print(text[:50])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "first citizen:\n",
            "before we proceed any further, hear\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yuvCJoq_2u8g"
      },
      "source": [
        "Tokenizing text with two methode\n",
        "\n",
        "first: (word_tokenize) \n",
        "\n",
        "it doesnot filter signs\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VfBP7H_A0fSp"
      },
      "source": [
        "data=word_tokenize(text=text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QQobt5Oo1C0w",
        "outputId": "a45ea08e-892b-459e-ac54-8c4b932f6854"
      },
      "source": [
        "print(data[:20])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['first', 'citizen', ':', 'before', 'we', 'proceed', 'any', 'further', ',', 'hear', 'me', 'speak', '.', 'all', ':', 'speak', ',', 'speak', '.', 'first']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZGYQ28Cv3deY"
      },
      "source": [
        "\n",
        "\n",
        "def filter_data(variable):\n",
        "    sequence=['-','\\’','!','”','#','$','%','&','(',')','*','+',',','.','/',':',';','<','=','>','?','@','[','\\\\',']','^','_','`','{','|','}','~','\\t','\\n']\n",
        "    if (variable in sequence):\n",
        "        return False\n",
        "    else:\n",
        "        return True\n",
        "\n",
        "filtered = filter(filter_data,data)\n",
        "data_2=list(filtered)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y5OT0I0o48gu",
        "outputId": "0f484258-258e-437b-aa8d-111b9d87eabe"
      },
      "source": [
        "print(data_2[:20])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['first', 'citizen', 'before', 'we', 'proceed', 'any', 'further', 'hear', 'me', 'speak', 'all', 'speak', 'speak', 'first', 'citizen', 'you', 'are', 'all', 'resolved', 'rather']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rq51uYbM6vnJ"
      },
      "source": [
        "second: (text_to_word_sequence) \n",
        "\n",
        "it filters signs too"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JOupWafQ1_4R"
      },
      "source": [
        "data2=text_to_word_sequence(text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "URdW_6SH2JE2",
        "outputId": "a35c84c9-fd1c-4ebd-a417-527950766c14"
      },
      "source": [
        "print(data2[:20])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['first', 'citizen', 'before', 'we', 'proceed', 'any', 'further', 'hear', 'me', 'speak', 'all', 'speak', 'speak', 'first', 'citizen', 'you', 'are', 'all', 'resolved', 'rather']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wFW4T2l46zWv"
      },
      "source": [
        "Filter stop word"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HpszemX0CAGa"
      },
      "source": [
        "data_2=[word for word in data_2 if not word in ['-','.']]\n",
        "Tokenize_data=[word for word in data_2 if not word in stopwords.words('english')]\n",
        "Tokenize_data2=[word for word in data2 if not word in stopwords.words('english')]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mxm70ukxFRvm",
        "outputId": "61b634fb-5c08-4f79-d8e8-948dcaf69b45"
      },
      "source": [
        "print(Tokenize_data[:20])\n",
        "print(Tokenize_data2[:20])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['first', 'citizen', 'proceed', 'hear', 'speak', 'speak', 'speak', 'first', 'citizen', 'resolved', 'rather', 'die', 'famish', 'resolved', 'resolved', 'first', 'citizen', 'first', 'know', 'caius']\n",
            "['first', 'citizen', 'proceed', 'hear', 'speak', 'speak', 'speak', 'first', 'citizen', 'resolved', 'rather', 'die', 'famish', 'resolved', 'resolved', 'first', 'citizen', 'first', 'know', 'caius']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ek81PdzQF40S"
      },
      "source": [
        "proces_data=\" \".join(Tokenize_data)\n",
        "proces_data2=\" \".join(Tokenize_data2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xNJiLkY0GeEk",
        "outputId": "b1771566-95cb-4c3b-add8-c430175f7a06"
      },
      "source": [
        "print(proces_data[:50])\n",
        "print(proces_data2[:50])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "first citizen proceed hear speak speak speak first\n",
            "first citizen proceed hear speak speak speak first\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xjTVlhr1edJe"
      },
      "source": [
        "Checking if two string match to just use one of them"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jLh9okj-cmGv",
        "outputId": "8db61ed9-d434-42fb-ae24-a54448f1dbf7"
      },
      "source": [
        "diff1=[word for word in proces_data if not word in proces_data2]\n",
        "diff2=[word for word in proces_data2 if not word in proces_data]\n",
        "print(diff1)\n",
        "print(diff2)\n",
        "print(proces_data == proces_data2)\n",
        "print(proces_data[:50])\n",
        "print(proces_data2[:50])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '.', '.', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '.', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '.', '-', '-', '-', '-', '-', '-', '-', '-', '.', '.', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '.', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '.', '-', '-', '-', '-', '-', '-', '.', '-', '-', '-', '-', '-', '.', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '.', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '.', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '.', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '.', '-', '.', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '.', '.', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '.', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '.', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '.', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '.', '-', '-', '-', '-', '-', '-', '-', '.', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '.', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '.', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '.', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '.', '-', '-', '-', '-', '.', '-', '.', '-', '-', '-', '-', '.', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '.', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '.', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '.', '-', '-', '-', '-', '-', '-', '-', '-', '-', '.', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '.', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '.', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '.', '-', '-', '-', '-', '-', '-', '.', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '.', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-']\n",
            "[]\n",
            "False\n",
            "first citizen proceed hear speak speak speak first\n",
            "first citizen proceed hear speak speak speak first\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dl7sfIsPhDVy"
      },
      "source": [
        "A simple way to this step uisng re lib"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qSBf50drdDnD",
        "outputId": "60ea476e-29ef-4a5f-d627-0d4eaff11957"
      },
      "source": [
        "import re\n",
        "words = re.split(r'\\W+', text)\n",
        "Tokenize_data3=[word for word in words if not word in stopwords.words('english')]\n",
        "proces_data3=\" \".join(Tokenize_data3)\n",
        "print(proces_data3[:50])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "first citizen proceed hear speak speak speak first\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V4S9_SfbickI"
      },
      "source": [
        "Another  simple way to this step uisng string lib"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CplzJUVYhXLA",
        "outputId": "b37791bf-ddd6-493f-b6dd-6a0a29b32c79"
      },
      "source": [
        "words2 = text.split()\n",
        "# remove punctuation from each word\n",
        "import string\n",
        "table = str.maketrans('', '', string.punctuation)\n",
        "stripped = [w.translate(table) for w in words2]\n",
        "Tokenize_data4=[word for word in stripped if not word in stopwords.words('english')]\n",
        "proces_data4=\" \".join(Tokenize_data4)\n",
        "print(proces_data4[:50])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "first citizen proceed hear speak speak speak first\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DYELcVH5iguY",
        "outputId": "3b641ecd-d97f-4730-b8d7-2c4edebbb924"
      },
      "source": [
        "diff3=[word for word in proces_data4 if not word in proces_data3]\n",
        "print(diff3)\n",
        "print(type(proces_data3))\n",
        "print(proces_data3==proces_data4)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[]\n",
            "<class 'str'>\n",
            "False\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fG2EpK2bjpCS"
      },
      "source": [
        "test=[word for word in proces_data if word.isalpha()]\n",
        "test2=\" \".join(test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hTBvVl-pj0L8",
        "outputId": "7cda6ddb-e403-497a-e181-7bec58117e5c"
      },
      "source": [
        "d=[word for word in proces_data3 if not word in test2]\n",
        "\n",
        "print(d)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ZTcBzmgl0aX"
      },
      "source": [
        "# In next step we are going to make training dataes \n",
        "So we pick one of the result of above model personaly i think using (text_to_word_sequence) is better because it keeps \" ' \" that change the meaning of sentence\n",
        "\n",
        "data=proces_data2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ySCLliDxmrZh"
      },
      "source": [
        "Data=proces_data2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a9DQEt3wnCnw"
      },
      "source": [
        "In this implementation we use character prediction\n",
        "\n",
        "data to characters\n",
        "\n",
        "I prefer to keep the space maybe model can figure out words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-m4QWMYEm2z5"
      },
      "source": [
        "chars=[char for char in Data]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UBmI-BB_nqBf",
        "outputId": "0809af44-e074-4ced-b18a-b99e9b11c132"
      },
      "source": [
        "print(chars[:20])\n",
        "print(len(chars))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['f', 'i', 'r', 's', 't', ' ', 'c', 'i', 't', 'i', 'z', 'e', 'n', ' ', 'p', 'r', 'o', 'c', 'e', 'e']\n",
            "698670\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JO-rST8Znxz-"
      },
      "source": [
        "Changing char to int"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uuPDNZoXnvUD",
        "outputId": "a60bc2d6-1892-4fc0-c98b-cbb2ae52a95b"
      },
      "source": [
        "vocab=sorted(set(chars))\n",
        "print(len(vocab))\n",
        "print(vocab)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "29\n",
            "[' ', \"'\", '3', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SinIS8yfoL0m"
      },
      "source": [
        "chr_to_vocab={char:id for id, char in enumerate(vocab)}\n",
        "vocab_to_char={id:char for id, char in enumerate(vocab)}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bifoqyljqo3Q",
        "outputId": "d8d58e82-7766-4a60-84ea-c2ba19c4f746"
      },
      "source": [
        "print(chr_to_vocab)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{' ': 0, \"'\": 1, '3': 2, 'a': 3, 'b': 4, 'c': 5, 'd': 6, 'e': 7, 'f': 8, 'g': 9, 'h': 10, 'i': 11, 'j': 12, 'k': 13, 'l': 14, 'm': 15, 'n': 16, 'o': 17, 'p': 18, 'q': 19, 'r': 20, 's': 21, 't': 22, 'u': 23, 'v': 24, 'w': 25, 'x': 26, 'y': 27, 'z': 28}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A7asif5QquZm"
      },
      "source": [
        "int_char=[chr_to_vocab[i] for i in chars]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0o0ABgRYq8kx",
        "outputId": "97a8eec3-effd-4658-c607-b087f9467a96"
      },
      "source": [
        "print(int_char[:50])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[8, 11, 20, 21, 22, 0, 5, 11, 22, 11, 28, 7, 16, 0, 18, 20, 17, 5, 7, 7, 6, 0, 10, 7, 3, 20, 0, 21, 18, 7, 3, 13, 0, 21, 18, 7, 3, 13, 0, 21, 18, 7, 3, 13, 0, 8, 11, 20, 21, 22]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TisjlDQhrGvt",
        "outputId": "4cf9dce6-81c6-4d63-b3d8-6537c4efae83"
      },
      "source": [
        "from math import ceil\n",
        "sequence_length=200\n",
        "#batch_size=ceil(len(chars)/sequence_length)\n",
        "batch_size=300\n",
        "print(\"sequence length is:\"+str(sequence_length))\n",
        "print(\"batch size is:\"+str(batch_size))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sequence length is:200\n",
            "batch size is:300\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BQ5oEqgesU6J"
      },
      "source": [
        "import numpy as np\n",
        "import random\n",
        "from keras.utils.np_utils import to_categorical \n",
        "\n",
        "def get_batches(data,sequence_length,vocab_len):\n",
        "  npdata=np.array(data)/vocab_len\n",
        "  batches=[]\n",
        "  x_train=[npdata[i:i+sequence_length] for i in range(len(data)-sequence_length-1)]\n",
        "  #x_trian2=[x_train[i]/vocab_len for i in range(len(x_trian))]\n",
        "  y_train=[npdata[i+1:i+sequence_length+1] for i in range(len(data)-sequence_length-1)]\n",
        "  batches=[(x,y) for x,y in zip(x_train,y_train)]\n",
        "\n",
        "  random.shuffle(batches)\n",
        "\n",
        "  xtrain=[x for (x,_) in batches]\n",
        "  ytrain=[y for (_,y) in batches]\n",
        "\n",
        "  return xtrain,ytrain\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gLfV6ONyvFmX"
      },
      "source": [
        "xtrain,ytrain=get_batches(int_char,sequence_length,len(vocab))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I5fH8dDcKP-6",
        "outputId": "c8082f92-e5e6-4c6a-831a-c4ce6eb6d7bc"
      },
      "source": [
        "print(str(xtrain[0][:3])+str(xtrain[0][197:]))\n",
        "print(str(ytrain[0][:3])+str(ytrain[0][197:]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.10344828 0.68965517 0.31034483][0.5862069  0.55172414 0.75862069]\n",
            "[0.68965517 0.31034483 0.24137931][0.55172414 0.75862069 0.10344828]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bqs0aTx6Kxv7"
      },
      "source": [
        "X=[np.append(xtrain[i*batch_size],xtrain[i*batch_size+1:i*batch_size+batch_size])for i in range(int(len(int_char)/batch_size))]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gi7BUwglNVYH"
      },
      "source": [
        "y=[np.append(ytrain[i*batch_size],ytrain[i*batch_size+1:i*batch_size+batch_size])for i in range(int(len(int_char)/batch_size))]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xDERKZUUQCHf",
        "outputId": "540fd0ef-a9ba-4d0d-f8c6-32bf45c5c097"
      },
      "source": [
        "print(len(X))\n",
        "print(y[2327].shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2328\n",
            "(60000,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4t9t-RoeFawW"
      },
      "source": [
        "if X[len(X)-1].shape[0] < batch_size*sequence_length:\n",
        "  X = X[:-1]\n",
        "  y = y[:-1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6KrrY3GHGDbj",
        "outputId": "f511d228-fb01-465f-9749-b9556471889f"
      },
      "source": [
        "print(len(X))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2328\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b0vAylANPiW3"
      },
      "source": [
        "X2=[np.reshape(X[i],(-1,sequence_length,1)) for i in range(len(X)-1)]\n",
        "y2=[np.reshape(y[i],(-1,sequence_length,1)) for i in range(len(y)-1)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P5_GHkhqQafI",
        "outputId": "2984b61e-8deb-4e82-d3d5-6231cbbc7b4f"
      },
      "source": [
        "print(y2[0].shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(300, 200, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y7HsbNfKGIPN"
      },
      "source": [
        "from tensorflow.keras.layers import Input,Embedding,LSTM,GlobalMaxPooling1D,Dense\n",
        "from tensorflow.keras.models import  Model\n",
        "\n",
        "def builed_model(vocab_size,sequence_len,batch_size,embedding_dim = 256,rnn_units = 1024):\n",
        "\n",
        "  i = Input(shape=(sequence_len,))\n",
        "  x = Embedding(vocab_size, embedding_dim)(i)\n",
        "  x = LSTM(rnn_units, return_sequences=True)(x)\n",
        "  #x = GlobalMaxPooling1D()(x)\n",
        "  x = Dense(vocab_size)(x)\n",
        "\n",
        "  model = Model(i, x)\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LpZg_Q1I6gml"
      },
      "source": [
        "import tensorflow as tf\n",
        "def build_model2(vocab_size, embedding_dim, rnn_units, batch_size):\n",
        "  model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(vocab_size, embedding_dim, batch_input_shape=[batch_size, None]),\n",
        "    LSTM(rnn_units, return_sequences=True, recurrent_initializer='glorot_uniform',recurrent_activation='sigmoid',stateful=True,), \n",
        "    #GlobalMaxPooling1D(),\n",
        "    tf.keras.layers.Dense(vocab_size)\n",
        "  ])\n",
        "\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rDXZ5PkZJ6UG"
      },
      "source": [
        "stm=builed_model(len(vocab),sequence_length,batch_size,256,1024)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WCla-G9t8EcT"
      },
      "source": [
        "stm2=build_model2(len(vocab), 256, 1024, batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "URK0jlBnKWIE",
        "outputId": "ec1e7988-03dd-4aef-dc5c-726fe3e94382"
      },
      "source": [
        "stm.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         [(None, 200)]             0         \n",
            "_________________________________________________________________\n",
            "embedding (Embedding)        (None, 200, 256)          7424      \n",
            "_________________________________________________________________\n",
            "lstm (LSTM)                  (None, 200, 1024)         5246976   \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 200, 29)           29725     \n",
            "=================================================================\n",
            "Total params: 5,284,125\n",
            "Trainable params: 5,284,125\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hofny0PP8c_Z",
        "outputId": "59b3f0f0-fa73-4455-9d0e-c8124d5e6b78"
      },
      "source": [
        "stm2.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (300, None, 256)          7424      \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                (300, None, 1024)         5246976   \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (300, None, 29)           29725     \n",
            "=================================================================\n",
            "Total params: 5,284,125\n",
            "Trainable params: 5,284,125\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IYIUAgiAME8d"
      },
      "source": [
        "test=stm(X2[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0QM6BOhIMf-z",
        "outputId": "e3d75829-c71d-4046-e912-41b4a11230df"
      },
      "source": [
        "print(type(test))\n",
        "print(test.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'tensorflow.python.framework.ops.EagerTensor'>\n",
            "(300, 200, 29)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tA1Q73lj9Xgg"
      },
      "source": [
        "test2=stm2(X2[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IhBtFpxl9dUM",
        "outputId": "423b0489-6ee2-438f-b988-8168a492107a"
      },
      "source": [
        "print(type(test2))\n",
        "print(test2.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'tensorflow.python.framework.ops.EagerTensor'>\n",
            "(300, 200, 29)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HMRQAnJuARvI"
      },
      "source": [
        "los = tf.keras.losses.sparse_categorical_crossentropy(y2[0], test2, from_logits=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nm73OBCGAdw8",
        "outputId": "04b4531c-1543-48b3-83bd-dc1e24da20f0"
      },
      "source": [
        "print(los)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(\n",
            "[[3.3694773 3.370997  3.3720465 ... 3.3722966 3.3722966 3.3722966]\n",
            " [3.3694773 3.370997  3.3720465 ... 3.3722966 3.3722966 3.3722966]\n",
            " [3.3694773 3.370997  3.3720465 ... 3.3722966 3.3722966 3.3722966]\n",
            " ...\n",
            " [3.3694773 3.370997  3.3720465 ... 3.3722966 3.3722966 3.3722966]\n",
            " [3.3694773 3.370997  3.3720465 ... 3.3722966 3.3722966 3.3722966]\n",
            " [3.3694773 3.370997  3.3720465 ... 3.3722966 3.3722966 3.3722966]], shape=(300, 200), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "59HBBT7bOXgw"
      },
      "source": [
        "\n",
        "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0NK8KgOaAsie"
      },
      "source": [
        "def compute_loss(labels, logits):\n",
        "  loss = tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
        "  # loss = tf.keras.losses.sparse_categorical_crossentropy('''TODO''', '''TODO''', from_logits=True) # TODO\n",
        "  return loss\n",
        "optimizer = tf.keras.optimizers.Adam()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zfZ_9yv8A5n2"
      },
      "source": [
        "@tf.function\n",
        "def train_step(x, y): \n",
        "  with tf.GradientTape() as tape:\n",
        "    y_hat = stm2(x)\n",
        "    loss = compute_loss(y, y_hat)\n",
        "  grads = tape.gradient(loss, stm2.trainable_variables)\n",
        "  optimizer.apply_gradients(zip(grads, stm2.trainable_variables))\n",
        "  return loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "3fDX-fhWBG9G",
        "outputId": "9a686143-f7fd-49c0-a3e4-a186c78df0b2"
      },
      "source": [
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "step_per_epoch=10\n",
        "epoch=50\n",
        "\n",
        "history = []\n",
        "\n",
        "for i in range(epoch):\n",
        "  idx = np.random.choice(len(X2), step_per_epoch)\n",
        "  count=0\n",
        "  for iter in idx:\n",
        "    x_batch=X2[iter]\n",
        "    y_batch = y2[iter]\n",
        "    loss = train_step(x_batch, y_batch)\n",
        "    history.append(loss.numpy().mean())\n",
        "    print('traning on step {0} of cpoch {1}'.format(count,i))\n",
        "    count+=1\n",
        "\n",
        "stm2.save_weights('test.h5')\n",
        "plt.plot(history)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "traning on step 0 of cpoch 0\n",
            "traning on step 0 of cpoch 0\n",
            "traning on step 0 of cpoch 0\n",
            "traning on step 0 of cpoch 0\n",
            "traning on step 0 of cpoch 0\n",
            "traning on step 0 of cpoch 0\n",
            "traning on step 0 of cpoch 0\n",
            "traning on step 0 of cpoch 0\n",
            "traning on step 0 of cpoch 0\n",
            "traning on step 0 of cpoch 0\n",
            "traning on step 0 of cpoch 1\n",
            "traning on step 0 of cpoch 1\n",
            "traning on step 0 of cpoch 1\n",
            "traning on step 0 of cpoch 1\n",
            "traning on step 0 of cpoch 1\n",
            "traning on step 0 of cpoch 1\n",
            "traning on step 0 of cpoch 1\n",
            "traning on step 0 of cpoch 1\n",
            "traning on step 0 of cpoch 1\n",
            "traning on step 0 of cpoch 1\n",
            "traning on step 0 of cpoch 2\n",
            "traning on step 0 of cpoch 2\n",
            "traning on step 0 of cpoch 2\n",
            "traning on step 0 of cpoch 2\n",
            "traning on step 0 of cpoch 2\n",
            "traning on step 0 of cpoch 2\n",
            "traning on step 0 of cpoch 2\n",
            "traning on step 0 of cpoch 2\n",
            "traning on step 0 of cpoch 2\n",
            "traning on step 0 of cpoch 2\n",
            "traning on step 0 of cpoch 3\n",
            "traning on step 0 of cpoch 3\n",
            "traning on step 0 of cpoch 3\n",
            "traning on step 0 of cpoch 3\n",
            "traning on step 0 of cpoch 3\n",
            "traning on step 0 of cpoch 3\n",
            "traning on step 0 of cpoch 3\n",
            "traning on step 0 of cpoch 3\n",
            "traning on step 0 of cpoch 3\n",
            "traning on step 0 of cpoch 3\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-56-3f0d1438fae0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mx_batch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0my_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'traning on step {0} of cpoch {1}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    853\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    854\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 855\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    856\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    857\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2941\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   2942\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 2943\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   2944\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2945\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1917\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1918\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1919\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1921\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    558\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 560\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    561\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 285
        },
        "id": "hPiCAtXSYS0L",
        "outputId": "e40476da-c1ac-46cf-ccc6-4700b99942f8"
      },
      "source": [
        "stm2.save_weights('test.h5')\n",
        "plt.plot(history)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f588f2049d0>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TJPMeagUrM2n",
        "outputId": "7b02b076-7297-46f2-e40a-662d96b4da28"
      },
      "source": [
        "generator=build_model2(len(vocab), 256, 1024, 1)\n",
        "generator.load_weights('test.h5')\n",
        "generator.build(tf.TensorShape([1, None]))\n",
        "\n",
        "generator.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_2 (Embedding)      (1, None, 256)            7424      \n",
            "_________________________________________________________________\n",
            "lstm_2 (LSTM)                (1, None, 1024)           5246976   \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (1, None, 29)             29725     \n",
            "=================================================================\n",
            "Total params: 5,284,125\n",
            "Trainable params: 5,284,125\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C5AaU6R-rWkY"
      },
      "source": [
        "def generate_text(model, start_string, generation_length=100):\n",
        "  input_eval = [chr_to_vocab[s] for s in start_string] \n",
        "  input_eval = tf.expand_dims(input_eval, 0)\n",
        "  text_generated = []\n",
        "  model.reset_states()\n",
        "\n",
        "  for i in range(generation_length):\n",
        "      predictions = model(input_eval)\n",
        "      predictions = tf.squeeze(predictions, 0)\n",
        "      predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
        "      input_eval = tf.expand_dims([predicted_id], 0)\n",
        "      text_generated.append(vocab_to_char[predicted_id]) \n",
        "  return (start_string + ''.join(text_generated))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N5Vp-TY8uQ4j"
      },
      "source": [
        "generated_text = generate_text(generator, start_string=\"romeo\", generation_length=100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zfKLm8ALgakV",
        "outputId": "1d4444ec-cb43-48d8-c114-fb35536d9a22"
      },
      "source": [
        "print(generated_text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "romeoseq                                                                                                 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-uj0IPXtgl1W"
      },
      "source": [
        "# Using tensorflow datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hNAs8vv9gsun"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers.experimental import preprocessing\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "import time"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zfjSFqgRo6nP"
      },
      "source": [
        "reading data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bCWqsCIOhXjz",
        "outputId": "e8cd2fae-003a-436c-e9cb-54311b62c08d"
      },
      "source": [
        "file_path=tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\n",
            "1122304/1115394 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GHQBa_vHiYhw"
      },
      "source": [
        "text = open(file_path, 'rb').read().decode(encoding='utf-8')"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dMk99FYEjGVe",
        "outputId": "9c992c82-a012-4245-baa1-ea3cb97a4ecf"
      },
      "source": [
        "print(f'size of the input text is {len(text)}')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "size of the input text is 1115394\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pDc6bmzFccID",
        "outputId": "3a045e38-89af-44de-88aa-29e4f64c7181"
      },
      "source": [
        "print(text[:250])"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u1h41nUkjR5P"
      },
      "source": [
        "vocab=sorted(set(text))"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VJMVns1Hjd89",
        "outputId": "81d0814d-250b-4e5a-c46c-3dbf2139d42e"
      },
      "source": [
        "print(f'{len(vocab)} unique characters')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "65 unique characters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L0-Jdb4bo-kW"
      },
      "source": [
        "functions for transition between char and ids"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u60b7P-Zjgy3"
      },
      "source": [
        "ids_from_chars = preprocessing.StringLookup(vocabulary=list(vocab))"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TW9I6C0skCgf"
      },
      "source": [
        "chars_from_ids = tf.keras.layers.experimental.preprocessing.StringLookup(vocabulary=ids_from_chars.get_vocabulary(), invert=True)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j_sxZy7FkQVx"
      },
      "source": [
        "def text_from_ids(ids):\n",
        "  return tf.strings.reduce_join(chars_from_ids(ids),axis=-1)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0kaVdZSfpKvv"
      },
      "source": [
        "transfer data's char to id"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hXQ91Tu4dZIX",
        "outputId": "56904d1e-d25d-4fd1-f0bc-65d4dce6716a"
      },
      "source": [
        "all_ids = ids_from_chars(tf.strings.unicode_split(text, 'UTF-8'))\n",
        "all_ids"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1115394,), dtype=int64, numpy=array([20, 49, 58, ..., 47, 10,  2])>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k4r2QdDupX8D"
      },
      "source": [
        "cearting dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u7WpNFSOnTmL",
        "outputId": "8704ed8f-2811-42bd-a2a6-f144b8ade52e"
      },
      "source": [
        "ids_dataset=tf.data.Dataset.from_tensor_slices(all_ids)\n",
        "ids_dataset"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<TensorSliceDataset shapes: (), types: tf.int64>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xy_GjASrnwZV",
        "outputId": "f1782042-327b-42cb-d780-7e4764ab2e36"
      },
      "source": [
        "for ids in ids_dataset.take(10):\n",
        "    print(chars_from_ids(ids).numpy().decode('utf-8'))"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "F\n",
            "i\n",
            "r\n",
            "s\n",
            "t\n",
            " \n",
            "C\n",
            "i\n",
            "t\n",
            "i\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X-p5z0Eio4Yf"
      },
      "source": [
        "seq_length = 100\n",
        "examples_per_epoch = len(text)//(seq_length+1)"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kQNX1Jr5peyu"
      },
      "source": [
        "slice dataset to sequences"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Q3qjdACpg5n"
      },
      "source": [
        "sequences=ids_dataset.batch(batch_size=seq_length+1,drop_remainder=True)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WwBNKBtvqbyw",
        "outputId": "74542a20-d176-47f7-f75f-a528fbd966e8"
      },
      "source": [
        "for seq in sequences.take(1):\n",
        "  print(chars_from_ids(seq))"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(\n",
            "[b'F' b'i' b'r' b's' b't' b' ' b'C' b'i' b't' b'i' b'z' b'e' b'n' b':'\n",
            " b'\\n' b'B' b'e' b'f' b'o' b'r' b'e' b' ' b'w' b'e' b' ' b'p' b'r' b'o'\n",
            " b'c' b'e' b'e' b'd' b' ' b'a' b'n' b'y' b' ' b'f' b'u' b'r' b't' b'h'\n",
            " b'e' b'r' b',' b' ' b'h' b'e' b'a' b'r' b' ' b'm' b'e' b' ' b's' b'p'\n",
            " b'e' b'a' b'k' b'.' b'\\n' b'\\n' b'A' b'l' b'l' b':' b'\\n' b'S' b'p' b'e'\n",
            " b'a' b'k' b',' b' ' b's' b'p' b'e' b'a' b'k' b'.' b'\\n' b'\\n' b'F' b'i'\n",
            " b'r' b's' b't' b' ' b'C' b'i' b't' b'i' b'z' b'e' b'n' b':' b'\\n' b'Y'\n",
            " b'o' b'u' b' '], shape=(101,), dtype=string)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2D14crDWd647"
      },
      "source": [
        "slice dataset to x and y sequences"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jtZScYzSdrRP"
      },
      "source": [
        "def split_input_target(sequence):\n",
        "    input_text = sequence[:-1]\n",
        "    target_text = sequence[1:]\n",
        "    return input_text, target_text"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qP4h6uTad4_B"
      },
      "source": [
        "dataset = sequences.map(split_input_target)"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6mCXMpLVeErb",
        "outputId": "7b6ff39b-56ab-4b37-ee57-e978daf9ac67"
      },
      "source": [
        "for input_example, target_example in dataset.take(1):\n",
        "    print(\"Input :\", text_from_ids(input_example).numpy())\n",
        "    print(\"Target:\", text_from_ids(target_example).numpy())"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input : b'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou'\n",
            "Target: b'irst Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HPjO5rFies7q"
      },
      "source": [
        "Create training batches"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e9D_h5vfeo6S",
        "outputId": "b560c726-11de-4d22-9efa-11be67539afe"
      },
      "source": [
        "BATCH_SIZE = 64\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "dataset = (\n",
        "    dataset\n",
        "    .shuffle(BUFFER_SIZE)\n",
        "    .batch(BATCH_SIZE, drop_remainder=True)\n",
        "    .prefetch(tf.data.experimental.AUTOTUNE))\n",
        "\n",
        "dataset"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<PrefetchDataset shapes: ((64, 100), (64, 100)), types: (tf.int64, tf.int64)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y7x2u1mqfHL7"
      },
      "source": [
        "Build The Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-A-O6NXgfGyB"
      },
      "source": [
        "# Length of the vocabulary in chars\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "# The embedding dimension\n",
        "embedding_dim = 256\n",
        "\n",
        "# Number of RNN units\n",
        "rnn_units = 1024"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3t2Hr7wSfN9d"
      },
      "source": [
        "class MyModel(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
        "    super().__init__(self)\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.gru = tf.keras.layers.GRU(rnn_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True)\n",
        "    self.dense = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "  def call(self, inputs, states=None, return_state=False, training=False):\n",
        "    x = inputs\n",
        "    x = self.embedding(x, training=training)\n",
        "    if states is None:\n",
        "      states = self.gru.get_initial_state(x)\n",
        "    x, states = self.gru(x, initial_state=states, training=training)\n",
        "    x = self.dense(x, training=training)\n",
        "\n",
        "    if return_state:\n",
        "      return x, states\n",
        "    else:\n",
        "      return x"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZGU_wvLXfg2z"
      },
      "source": [
        "model = MyModel(\n",
        "    vocab_size=len(ids_from_chars.get_vocabulary()),\n",
        "    embedding_dim=embedding_dim,\n",
        "    rnn_units=rnn_units)"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cMxFWk4XfqGb",
        "outputId": "6e5c8506-37bd-46e1-bdba-31d1c540293a"
      },
      "source": [
        "for input_example_batch, target_example_batch in dataset.take(1):\n",
        "    example_batch_predictions = model(input_example_batch)\n",
        "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(64, 100, 67) # (batch_size, sequence_length, vocab_size)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Paw5HO0tfwCJ",
        "outputId": "3ba094e6-5de1-4813-b807-4641498aa241"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"my_model_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      multiple                  17152     \n",
            "_________________________________________________________________\n",
            "gru_1 (GRU)                  multiple                  3938304   \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              multiple                  68675     \n",
            "=================================================================\n",
            "Total params: 4,024,131\n",
            "Trainable params: 4,024,131\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hGX7-K5Jfyop",
        "outputId": "c2c50193-5be0-49da-b493-3a1a29424788"
      },
      "source": [
        "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
        "sampled_indices = tf.squeeze(sampled_indices, axis=-1).numpy()\n",
        "sampled_indices"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([11, 12, 56,  6,  9, 57,  5, 54, 59, 40, 34, 40, 17,  4,  8, 25, 37,\n",
              "       28, 47, 46, 26, 20, 63, 53,  3, 37, 49, 27, 54, 21,  0, 41, 16, 19,\n",
              "       23, 47, 30,  6, 47, 18, 28, 64,  5, 38, 58, 21, 24, 28, 11,  2, 21,\n",
              "       58, 15, 39, 15, 36, 48, 65, 57, 32,  8, 31, 29, 40,  9, 53, 28,  0,\n",
              "       60, 45, 38, 38, 15, 17, 55,  1, 28, 37, 38, 13, 37, 22, 66, 34, 25,\n",
              "       26, 30, 26, 36, 16, 42, 40, 12, 16,  2, 12, 42, 48, 39, 51])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6acwMYNMf-nl",
        "outputId": "7a067fbc-d286-451b-a4f7-f42718c5c6c5"
      },
      "source": [
        "print(\"Input:\\n\", text_from_ids(input_example_batch[0]).numpy())\n",
        "print()\n",
        "print(\"Next Char Predictions:\\n\", text_from_ids(sampled_indices).numpy())"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input:\n",
            " b\" a'\\nvictory in his pocket? the wounds become him.\\n\\nVOLUMNIA:\\nOn's brows: Menenius, he comes the thir\"\n",
            "\n",
            "Next Char Predictions:\n",
            " b'3:p&-q$nsZTZC!,KWNgfLFwm WiMnGaBEIgP&gDNx$XrGJN3\\nGrAYAVhyqR,QOZ-mNteXXACo[UNK]NWX;WHzTKLPLVBbZ:B\\n:bhYk'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5X1l82pjgToI"
      },
      "source": [
        "Train the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LQGGxTk6gDRD"
      },
      "source": [
        "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "caBrqzpMgUhY",
        "outputId": "799e691b-2df6-4a0c-adfa-dd79041f7d5a"
      },
      "source": [
        "example_batch_loss = loss(target_example_batch, example_batch_predictions)\n",
        "mean_loss = example_batch_loss.numpy().mean()\n",
        "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
        "print(\"Mean loss:        \", mean_loss)"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Prediction shape:  (64, 100, 67)  # (batch_size, sequence_length, vocab_size)\n",
            "Mean loss:         4.2050505\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VoU3R9YZgctd"
      },
      "source": [
        "A newly initialized model shouldn't be too sure of itself, the output logits should all have similar magnitudes. To confirm this you can check that the exponential of the mean loss is approximately equal to the vocabulary size. A much higher loss means the model is sure of its wrong answers, and is badly initialized:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "INFgn_i7gblk",
        "outputId": "f7696c49-c930-4c4a-af64-fce878b1dcb2"
      },
      "source": [
        "tf.exp(mean_loss).numpy()"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "67.02398"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xQOMLhFMgjXa"
      },
      "source": [
        "model.compile(optimizer='adam', loss=loss)"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ewX4A9ZmgmDP"
      },
      "source": [
        "# Directory where the checkpoints will be saved\n",
        "checkpoint_dir = '/content/training_checkpoints'\n",
        "# Name of the checkpoint files\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True)"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rfJZg38egy1F"
      },
      "source": [
        "EPOCHS = 20"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 417
        },
        "id": "M8uTng48g2n0",
        "outputId": "2dbaf72a-142c-4ea3-f4ba-7b8e57842970"
      },
      "source": [
        "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "172/172 [==============================] - 914s 5s/step - loss: 3.2850\n",
            "Epoch 2/20\n",
            "  7/172 [>.............................] - ETA: 14:36 - loss: 2.1613"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-52-25e345c13e8b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcheckpoint_callback\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1098\u001b[0m                 _r=1):\n\u001b[1;32m   1099\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1100\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1101\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    853\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    854\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 855\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    856\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    857\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2941\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   2942\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 2943\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   2944\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2945\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1917\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1918\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1919\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1921\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    558\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 560\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    561\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V3TuvZx9lVR7"
      },
      "source": [
        "Generate text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aym1uVnukmlj"
      },
      "source": [
        "class OneStep(tf.keras.Model):\n",
        "  def __init__(self, model, chars_from_ids, ids_from_chars, temperature=1.0):\n",
        "    super().__init__()\n",
        "    self.temperature = temperature\n",
        "    self.model = model\n",
        "    self.chars_from_ids = chars_from_ids\n",
        "    self.ids_from_chars = ids_from_chars\n",
        "\n",
        "    # Create a mask to prevent \"\" or \"[UNK]\" from being generated.\n",
        "    skip_ids = self.ids_from_chars(['', '[UNK]'])[:, None]\n",
        "    sparse_mask = tf.SparseTensor(\n",
        "        # Put a -inf at each bad index.\n",
        "        values=[-float('inf')]*len(skip_ids),\n",
        "        indices=skip_ids,\n",
        "        # Match the shape to the vocabulary\n",
        "        dense_shape=[len(ids_from_chars.get_vocabulary())])\n",
        "    self.prediction_mask = tf.sparse.to_dense(sparse_mask)\n",
        "\n",
        "  @tf.function\n",
        "  def generate_one_step(self, inputs, states=None):\n",
        "    # Convert strings to token IDs.\n",
        "    input_chars = tf.strings.unicode_split(inputs, 'UTF-8')\n",
        "    input_ids = self.ids_from_chars(input_chars).to_tensor()\n",
        "\n",
        "    # Run the model.\n",
        "    # predicted_logits.shape is [batch, char, next_char_logits]\n",
        "    predicted_logits, states = self.model(inputs=input_ids, states=states,\n",
        "                                          return_state=True)\n",
        "    # Only use the last prediction.\n",
        "    predicted_logits = predicted_logits[:, -1, :]\n",
        "    predicted_logits = predicted_logits/self.temperature\n",
        "    # Apply the prediction mask: prevent \"\" or \"[UNK]\" from being generated.\n",
        "    predicted_logits = predicted_logits + self.prediction_mask\n",
        "\n",
        "    # Sample the output logits to generate token IDs.\n",
        "    predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
        "    predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n",
        "\n",
        "    # Convert from token ids to characters\n",
        "    predicted_chars = self.chars_from_ids(predicted_ids)\n",
        "\n",
        "    # Return the characters and model state.\n",
        "    return predicted_chars, states"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iJlXPo5hlbKR"
      },
      "source": [
        "one_step_model = OneStep(model, chars_from_ids, ids_from_chars)"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eIBkDFJ0lk85",
        "outputId": "6c7f3e5d-3822-476e-9572-df26c7a98355"
      },
      "source": [
        "start = time.time()\n",
        "states = None\n",
        "next_char = tf.constant(['ROMEO:'])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(1000):\n",
        "  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
        "  result.append(next_char)\n",
        "\n",
        "result = tf.strings.join(result)\n",
        "end = time.time()\n",
        "print(result[0].numpy().decode('utf-8'), '\\n\\n' + '_'*80)\n",
        "print('\\nRun time:', end - start)"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ROMEO:\n",
            "I crsathid sirle my whave ure aid mathinglyo is of the menth sut ntour and le peratteris hestor I cabeat ware thas gither the broulllle.\n",
            "Ther I goum, all tho ksce wiwh dic, I hant me soes will mo; surk peact and hith sa dore thing?\n",
            "\n",
            "OREG:\n",
            "Thire have not our hither meis tlay may\n",
            "Richiniten's wise, there; I wioe wous Love reovy sootn?\n",
            "\n",
            "There ame te thave dires.\n",
            "\n",
            "Kit, me prvert.\n",
            "\n",
            "CLORUNOS:\n",
            "I lelly kid\n",
            "Berlaed coartow anes famd ie fotuls s riprlectien hou beve faris;\n",
            "Cofle fours thours moten-farso mid, of ipf of fore, hig.\n",
            "\n",
            "ICESENL\n",
            "DE:\n",
            "Ande bryouchirndoue,\n",
            "Now toy foak,\n",
            "Torreare thou a lltirxe srenger'd srot?\n",
            "The aitar airk dpating tove yourt thours myoul, ald; har tall? Mysellerd, cowlid sowe inovere, I sulenely hir be fos toeruthy thar dithy, bund cercerery, ay thea Hamly.\n",
            "Sursed llatis, Cafin burandststoy cit pent, as siepen ke ortt ragnce totho weengrere faread, Memn,\n",
            "St, en hathind's; myor to thenm\n",
            "Yir ford enopsthes ald bugr to delly co ma-dein may, with prtiend?\n",
            "\n",
            "TENOUS:\n",
            "Oy I hand, \n",
            "\n",
            "________________________________________________________________________________\n",
            "\n",
            "Run time: 3.2145402431488037\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hTs-wo2ploLh",
        "outputId": "563ff0ea-5227-423b-d700-e8dce72e408c"
      },
      "source": [
        "start = time.time()\n",
        "states = None\n",
        "next_char = tf.constant(['ROMEO:', 'ROMEO:', 'ROMEO:', 'ROMEO:', 'ROMEO:'])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(1000):\n",
        "  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
        "  result.append(next_char)\n",
        "\n",
        "result = tf.strings.join(result)\n",
        "end = time.time()\n",
        "print(result, '\\n\\n' + '_'*80)\n",
        "print('\\nRun time:', end - start)"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(\n",
            "[b\"ROMEO:\\nMe coan fus hert.\\n\\nHONNIO:\\nWhyowe hore he ther breato,\\nBe tlond? And hay the heok, I hur, thilldwer ande treme,\\nWhenw thet,\\nWhalin galt\\nMy ulowe dery and lir if and anff ingor?\\nHomy hoth gat mone.\\nBurpind cucpe-ceie'dod af af hirnder\\nTith mabe meree-arith, that le if head\\nPrreovevinswe'd swich it andule\\nAs sirn.\\nMame, ando moll nevit ney ande, so a pones thee agrr; him us fotr;\\nAn the wireass.\\nWhimn foog thin pawant tint ond frochereand it itear'd math itba bisting, mavenaren,\\nIf this coulo co fime my cous selino, buld nopn not rot ourd, axe neom'; I'd, itben'ldt;\\nThent ut los.\\n\\nDURENI:\\nCot rishon-\\nQUEETUUCEI:\\nA brow mead.\\n\\nMARELI ELA:\\nCorde,;\\nI his noy stleme csoe it pris perry, meod;\\nI de afof bo my gheidt toy garntocbersc; bpary hrent.\\n\\nVANK EOT INUK:\\nMert.\\nWher comey, to the, farant; and wang'led ods io gerrvathe spees!\\n\\nHill that I my revare, Mard; fis I's he as, heave\\nHell Werenotingr bingadd he leno toverd tou rimerid.\\nThe br hivergrin therur of thes cearont, ryous ris the prim\"\n",
            " b\"ROMEO:\\nFou lizenun O, the hant wathen thaus un; yout southerk rime eathog then-brot'dirves\\nTy grat in maradw,\\nArdur my treall forcwouf ghink\\nAnded peore ligheds bromy ene't iven you il her gimy.\\n\\nASTAs INONBELT:\\nThat that sewkine vevtith\\nI suly touron.\\nThut whou hisp, o lavot your te 'ave dis y haty, hot no ke'plars you fraghear suverur'dy ptont at minc'cG cuith\\nTh eldersund man of the tie it in the nesad in ancone, tero' tore I lapkned,\\nAnd hie hasse fard tre geath in his oo ftick hy\\nAs pade IwAmy,\\nIf, lighy horns aw tle lim, non sotanomtilld. I, witcon,\\nHavine?\\nNoutherd tath espo my cooumingen wurs:\\nAnd eurt tht be to not and mr\\nMeep ou keloust com cence treand lofnor ne to gay, lusels and uld ond your not soroum-for dy coobne,\\nWhow inw thoe stald, net I marnot ros an forred, onis horis bestadd thing, tove ktowind hos aprent tirer, it kean them;\\nIs sackstere tote towl braivern.\\nBut Fotke ardser\\nMicly? Hes mun youl cemey:\\nI till nrath peesth bets ow.\\nWhemy bust the! wale carme yore,, of for\"\n",
            " b\"ROMEO:\\nWhoy frabecss.\\n\\nKOLIN IINTUS:\\nI vecos thit soml olp, reais than by tot.\\nFry thin ther of ingiroNman: tope,\\nTilp of ond lorp hom whaid ked sonat ceaked bot.\\nG'dt I E wicy toug ome thour am ceing; tor E fomrore my toud dith bented:\\nMe tromn sh yould,\\nA'ds-\\nAxpocery.\\nByor mot uth thled lfare.\\n\\nBOMNIO:\\nIow not not what and co hes srountongo; I houne!\\n\\nHe hads thel chire,\\nI wh\\nI lowh prererad thin sinst havevs ay hithis is fortirers,\\nAn hithe gider son the carpo'd ig thid southe,\\nWhtheuls thas way ssul.\\n\\nCWARH:\\nHeesthom heat; thou govest\\nWhat I haked go tfarpe an kicr.\\nTht lice!\\nTo rereoy sill amllo; whous sod setor, so meld le songe of tor.\\n\\nRUMIO:\\nAid thet und he risl, thaus:\\nOnden:\\nAne, tht mutretlen.\\n\\nARIUNIN:\\nIw thou s atssert be nou merd fem.\\nNow, airtserf hivieg's bloprishunf bertowe teos sitheie.\\nAf-CKerem th ciigh pleadf: Gof coun powsistbt To lyour theire ale quland, I knen\\nAw it hingerd my midneestre ie nom\\n\\nBINDUD ED:\\nUu mom, ghath'd mathing:\\nBy tor prow trod ma kpady! Bulspy i\"\n",
            " b\"ROMEO:\\nI lakd hom En me noms,\\nI pavesmeefor hith boy lith mend bat foung'dar.,\\nMard.\\nAs sook erteer not bero ttor quegatnore 's urd of earls an:\\nblit vath sthy toy soun satherentt thas, buns chear bend my\\nHepthly:\\nNor fot be oulurbnedirede four is levece go thate dove tha woly, whak?\\nOfts shester in toun.\\n\\nBAd, ghein no be dotrise omcow deaif irt your hed ifwy the thas, erae itid.\\n\\nKONHAMD:\\nBy botordestrero nay thing. tr cefeathed ben yours--Xand sto loun nou inde, bres tor lering youres, thet I'D\\nHe ger neath\\nEnd meing ald, tive hed beegrot to briverer:\\nTor theml allad ofr, tho my serling o whe dor bist, wir youwese.\\nY rone wethet\\nI chor dotens, thtr pomsis\\nYorn fale if to corar;\\nTuwbis ast battel rithan,\\nAnd bo mand poor.\\n\\nFith\\nTe sply madtengcy the Selow, unco, surte't Lorce ie ther hoverurre.\\nTher themus mane drouse:\\nMasint sis roun shour what she cis, then, 'sich Iicher, Onllm hend as Lead, she afroder, and thoo, whe serich srat her onl;\\nAnd bershis ghit ceonmy nov wave's J'oa bent my y\"\n",
            " b\"ROMEO:\\nPros bry, shine woble.\\n\\nCUCIOSTA:\\nOl wilk cets, mee, th, yous veecele, nate; ane penofrell-fee, myon go ager.\\n\\nTONING:\\nHlery dome's ho kim on: tow with my up hep.\\n\\nQUKINV:\\nWeain lave cayked fowes,\\nAnd my see foo, front?\\nHor, wiml! I lise frowl, le havest, Limk stint and myous on whin thy ewablingan\\nI rath oe's: me to thou his snebe ald bence fartsedsigs the mand Corbepighve my is hishars.\\nTha d thie maven; jiras.\\n\\nJUCANTIUGI:\\nWell anm meapbay los hon th.\\nButtand!\\nOndrage of lerthoul apat, will keak bist chourn ina morcar?\\n\\nGRUANTGRRO:\\nNo wancid seows ofe be cone;, with be yee,\\nInt the, de io In ppored bonculim ke thatl, twill buthere madp IM'd tous srote mith's fathingt of yousingare farinstes asbaly,!\\nTiRI VINNA,\\nBE HINCKARI CBMRORYILD:\\nPay hitht Ard th thit your heincherid thell dith thele ale hash breor\\nIten than tre gaun, be, frouchath\\nMicher hist wo my brurowand le tho hingoun\\nLar Loke, heachery thoubrougr pera, Nur o verec'st tom od stis list ind hrow rout perpain\\nin ledd; nit h\"], shape=(5,), dtype=string) \n",
            "\n",
            "________________________________________________________________________________\n",
            "\n",
            "Run time: 5.0299904346466064\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r7FRqKz1lwzD"
      },
      "source": [
        "Export the generator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aaC-9tS3lzFw",
        "outputId": "a388b758-e01d-43e6-fb7e-5282f81127ff"
      },
      "source": [
        "tf.saved_model.save(one_step_model, 'one_step')\n",
        "one_step_reloaded = tf.saved_model.load('one_step')"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Skipping full serialization of Keras layer <__main__.OneStep object at 0x7f3f524f4850>, because it is not built.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as gru_cell_1_layer_call_and_return_conditional_losses, gru_cell_1_layer_call_fn, gru_cell_1_layer_call_fn, gru_cell_1_layer_call_and_return_conditional_losses, gru_cell_1_layer_call_and_return_conditional_losses while saving (showing 5 of 5). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as gru_cell_1_layer_call_and_return_conditional_losses, gru_cell_1_layer_call_fn, gru_cell_1_layer_call_fn, gru_cell_1_layer_call_and_return_conditional_losses, gru_cell_1_layer_call_and_return_conditional_losses while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: one_step/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: one_step/assets\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bT_l9G-ol6OJ",
        "outputId": "7e28a469-d002-4f01-e1e5-796147a4688b"
      },
      "source": [
        "states = None\n",
        "next_char = tf.constant(['ROMEO:'])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(100):\n",
        "  next_char, states = one_step_reloaded.generate_one_step(next_char, states=states)\n",
        "  result.append(next_char)\n",
        "\n",
        "print(tf.strings.join(result)[0].numpy().decode(\"utf-8\"))"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:5 out of the last 5 calls to <function recreate_function.<locals>.restored_function_body at 0x7f3f51b7ef80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:5 out of the last 5 calls to <function recreate_function.<locals>.restored_function_body at 0x7f3f51b7ef80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:6 out of the last 6 calls to <function recreate_function.<locals>.restored_function_body at 0x7f3f51b7ef80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:6 out of the last 6 calls to <function recreate_function.<locals>.restored_function_body at 0x7f3f51b7ef80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "ROMEO:\n",
            "No karl feak.\n",
            "Wyou; I'g teor corthy, ad wale pelivevord of so ruks pooe, atmer?\n",
            "'st socher meadt, f\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ecM7PPlFl_jQ"
      },
      "source": [
        "Advanced: Customized Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pNozY_bFmCJz"
      },
      "source": [
        "class CustomTraining(MyModel):\n",
        "  @tf.function\n",
        "  def train_step(self, inputs):\n",
        "      inputs, labels = inputs\n",
        "      with tf.GradientTape() as tape:\n",
        "          predictions = self(inputs, training=True)\n",
        "          loss = self.loss(labels, predictions)\n",
        "      grads = tape.gradient(loss, model.trainable_variables)\n",
        "      self.optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "\n",
        "      return {'loss': loss}"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MliJnJz_mJas"
      },
      "source": [
        "model = CustomTraining(\n",
        "    vocab_size=len(ids_from_chars.get_vocabulary()),\n",
        "    embedding_dim=embedding_dim,\n",
        "    rnn_units=rnn_units)"
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OmAZEXqimM3f"
      },
      "source": [
        "model.compile(optimizer = tf.keras.optimizers.Adam(),\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True))"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DCP7XpQCmOfx",
        "outputId": "745353dd-8772-454a-fcb0-6b52bb606dae"
      },
      "source": [
        "model.fit(dataset, epochs=1)"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "172/172 [==============================] - 902s 5s/step - loss: 2.7336\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f3f51256190>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e93MlMqXmScO"
      },
      "source": [
        "Or if you need more control, you can write your own complete custom training loop:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "SGEiaDksmTKJ",
        "outputId": "5e27c080-eea5-4343-b7b1-001790a11040"
      },
      "source": [
        "EPOCHS = 10\n",
        "\n",
        "mean = tf.metrics.Mean()\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    start = time.time()\n",
        "\n",
        "    mean.reset_states()\n",
        "    for (batch_n, (inp, target)) in enumerate(dataset):\n",
        "        logs = model.train_step([inp, target])\n",
        "        mean.update_state(logs['loss'])\n",
        "\n",
        "        if batch_n % 50 == 0:\n",
        "            template = f\"Epoch {epoch+1} Batch {batch_n} Loss {logs['loss']:.4f}\"\n",
        "            print(template)\n",
        "\n",
        "    # saving (checkpoint) the model every 5 epochs\n",
        "    if (epoch + 1) % 5 == 0:\n",
        "        model.save_weights(checkpoint_prefix.format(epoch=epoch))\n",
        "\n",
        "    print()\n",
        "    print(f'Epoch {epoch+1} Loss: {mean.result().numpy():.4f}')\n",
        "    print(f'Time taken for 1 epoch {time.time() - start:.2f} sec')\n",
        "    print(\"_\"*80)\n",
        "\n",
        "model.save_weights(checkpoint_prefix.format(epoch=epoch))"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 Batch 0 Loss 2.1924\n",
            "Epoch 1 Batch 50 Loss 2.0344\n",
            "Epoch 1 Batch 100 Loss 1.9313\n",
            "Epoch 1 Batch 150 Loss 1.8792\n",
            "\n",
            "Epoch 1 Loss: 1.9998\n",
            "Time taken for 1 epoch 917.21 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 2 Batch 0 Loss 1.8030\n",
            "Epoch 2 Batch 50 Loss 1.7777\n",
            "Epoch 2 Batch 100 Loss 1.6840\n",
            "Epoch 2 Batch 150 Loss 1.6447\n",
            "\n",
            "Epoch 2 Loss: 1.7179\n",
            "Time taken for 1 epoch 903.79 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 3 Batch 0 Loss 1.6089\n",
            "Epoch 3 Batch 50 Loss 1.5718\n",
            "Epoch 3 Batch 100 Loss 1.5399\n",
            "Epoch 3 Batch 150 Loss 1.5081\n",
            "\n",
            "Epoch 3 Loss: 1.5536\n",
            "Time taken for 1 epoch 893.52 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 4 Batch 0 Loss 1.4677\n",
            "Epoch 4 Batch 50 Loss 1.4385\n",
            "Epoch 4 Batch 100 Loss 1.4463\n",
            "Epoch 4 Batch 150 Loss 1.4936\n",
            "\n",
            "Epoch 4 Loss: 1.4534\n",
            "Time taken for 1 epoch 908.19 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 5 Batch 0 Loss 1.3975\n",
            "Epoch 5 Batch 50 Loss 1.4065\n",
            "Epoch 5 Batch 100 Loss 1.3865\n",
            "Epoch 5 Batch 150 Loss 1.3463\n",
            "\n",
            "Epoch 5 Loss: 1.3858\n",
            "Time taken for 1 epoch 895.43 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 6 Batch 0 Loss 1.2903\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-63-c09e36422b17>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mmean\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_states\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbatch_n\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0mmean\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    853\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    854\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 855\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    856\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    857\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2941\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   2942\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 2943\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   2944\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2945\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1917\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1918\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1919\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1921\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    558\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 560\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    561\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}