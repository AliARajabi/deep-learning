{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "machine translation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNSaKAar9+0wgThtpy7lkwL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AliARajabi/deep-learning/blob/main/RNN/machine_translation(Uncompelet).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x7-nSnTImzIJ"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import unicodedata\n",
        "import re\n",
        "import numpy as np\n",
        "import os\n",
        "import io\n",
        "import time"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WMyeFmTapuaX"
      },
      "source": [
        "Download data from [manythings](https://www.manythings.org/anki/pes-eng.zip)\n",
        "this link is forbidden from our country so we download it manually"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BhOaT5HenrSu",
        "outputId": "acdee044-8267-450e-aec8-ca4ea2c63a8e"
      },
      "source": [
        "!wget http://www.manythings.org/anki/pes-eng.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-04-16 16:58:49--  http://www.manythings.org/anki/pes-eng.zip\n",
            "Resolving www.manythings.org (www.manythings.org)... 104.21.55.222, 172.67.173.198, 2606:4700:3031::6815:37de, ...\n",
            "Connecting to www.manythings.org (www.manythings.org)|104.21.55.222|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 112927 (110K) [application/zip]\n",
            "Saving to: ‘pes-eng.zip’\n",
            "\n",
            "pes-eng.zip         100%[===================>] 110.28K  --.-KB/s    in 0.1s    \n",
            "\n",
            "2021-04-16 16:58:50 (841 KB/s) - ‘pes-eng.zip’ saved [112927/112927]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P3PMhQTYpLnf",
        "outputId": "f922f5f3-4f99-418a-d133-cdf1ab576f29"
      },
      "source": [
        "! unzip /content/pes-eng.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  /content/pes-eng.zip\n",
            "  inflating: _about.txt              \n",
            "  inflating: pes.txt                 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mUeluoJrm4o3"
      },
      "source": [
        "file_path='/content/pes.txt'\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YGkmUF8XrNEu"
      },
      "source": [
        "Converts the unicode file to ascii"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X94AGiYmptkX"
      },
      "source": [
        "def Converts_unicode_to_ascii(text):\n",
        "  return ''.join(c for c in unicodedata.normalize('NFD', text)if unicodedata.category(c) != 'Mn')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "29oSjejosWok"
      },
      "source": [
        "def preprocess_sentence(w):\n",
        "  w = Converts_unicode_to_ascii(w.lower().strip())\n",
        "\n",
        "  w = re.sub(r\"([?.!,¿])\", r\" \\1 \", w)\n",
        "  w = re.sub(r'[\" \"]+', \" \", w)\n",
        "\n",
        "  # replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\")\n",
        "  w = re.sub(r\"[^a-zA-Z?.!,؟(آابپتثجچحخدذرزژسشصضطظعغفقکگلمنوهی)]+\", \" \", w)\n",
        "\n",
        "  w = w.strip()\n",
        "\n",
        "  # adding a start and an end token to the sentence\n",
        "  # so that the model know when to start and stop predicting.\n",
        "  w = '<start> ' + w + ' <end>'\n",
        "  return w"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qcaf5tOZuUcb",
        "outputId": "eec9809d-8070-4cbe-b85c-16e7c3ceb0bd"
      },
      "source": [
        "en_sentence = u\"May I borrow this book?\"\n",
        "sp_sentence = u\"       ¿می تونم این کتاب روِِِِِِِِِیِِِِِِِِِِِ|||| قرض بگیرم؟\"\n",
        "print(preprocess_sentence(en_sentence))\n",
        "print(preprocess_sentence(sp_sentence))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<start> may i borrow this book ? <end>\n",
            "<start> می تونم این کتاب روی قرض بگیرم؟ <end>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A8m6HZpUwiEA"
      },
      "source": [
        "def Create_dataset(path,data_num):\n",
        "  lines=io.open(file=path,encoding='UTF-8').read().strip().split('\\n')\n",
        "  words=[[preprocess_sentence(s) for s in line.split('\\t')] for line in lines[:data_num]]\n",
        "  return zip(*words)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B58R-wsp7Upq",
        "outputId": "f05b9289-40cf-4b2d-b511-dcebccb1ebf2"
      },
      "source": [
        "en, per,_ = Create_dataset(file_path, 3)\n",
        "print(en)\n",
        "print(per)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "('<start> who ? <end>', '<start> go on . <end>', '<start> smile . <end>')\n",
            "('<start> چه کسی؟ <end>', '<start> ادامه بده ( ادامه دادن ) <end>', '<start> لبخند بزن . <end>')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F1Hri_lz9Veq"
      },
      "source": [
        "def tokenize(inp):\n",
        "  tok=tf.keras.preprocessing.text.Tokenizer(filters='')\n",
        "  tok.fit_on_texts(inp)\n",
        "\n",
        "  tensor=tok.texts_to_sequences(inp)\n",
        "  tensor=tf.keras.preprocessing.sequence.pad_sequences(tensor,padding='post')\n",
        "\n",
        "  return tensor,tok"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dwYjI70SHRzG"
      },
      "source": [
        "def load_dataset(path,data_num):\n",
        "  input_lang,output_lang,_=Create_dataset(path, data_num)\n",
        "\n",
        "  input_lang_tok,input_tokenizer=tokenize(input_lang)\n",
        "  output_lang_tok,output_tokenizer=tokenize(output_lang)\n",
        "  return input_lang_tok,output_lang_tok,input_tokenizer,output_tokenizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K9_RAsflIZ5c"
      },
      "source": [
        "num_examples = 30000\n",
        "input_tensor, target_tensor, inp_lang, targ_lang = load_dataset(file_path,\n",
        "                                                                num_examples)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PVCsNF1QIuEW",
        "outputId": "1398ae76-9468-485a-f661-15776998b96d"
      },
      "source": [
        "print(type(input_tensor))\n",
        "print(input_tensor.shape)\n",
        "print(input_tensor[50])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'numpy.ndarray'>\n",
            "(2284, 37)\n",
            "[  1  17  18 264   3   2   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ufEPa8JBJ_g-",
        "outputId": "8ab92f03-6552-4317-9c5c-67c99963533a"
      },
      "source": [
        "print(type(target_tensor))\n",
        "print(target_tensor.shape)\n",
        "print(target_tensor[50])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'numpy.ndarray'>\n",
            "(2284, 34)\n",
            "[  1 106   9   3   2   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MIkZCAocJuPX"
      },
      "source": [
        "max_length_targ, max_length_inp = target_tensor.shape[1], input_tensor.shape[1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MscO8vATKbrg"
      },
      "source": [
        "input_tensor_train,input_tensor_val,target_tensor_train,target_tensor_val = train_test_split(input_tensor, target_tensor,test_size=0.2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YK58fFlULGsh",
        "outputId": "6a12144e-677e-41b9-8cda-7011d2febb94"
      },
      "source": [
        "print(f'input train size: {len(input_tensor_train)}, output train size: {len(target_tensor_train)}, input validation size: {len(input_tensor_val)},output validation size: {len(target_tensor_val)}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "input train size: 1827, output train size: 1827, input validation size: 457,output validation size: 457\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DmjkZ8tFOzgG"
      },
      "source": [
        "def convert(lang, tensor):\n",
        "  for t in tensor:\n",
        "    if t != 0:\n",
        "      print(f'{t} ----> {lang.index_word[t]}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qnt7GkcDO9KN",
        "outputId": "da0e72df-20c7-4feb-8984-61a371dc0ab6"
      },
      "source": [
        "print(\"Input Language; index to word mapping\")\n",
        "convert(inp_lang, input_tensor_train[0])\n",
        "print()\n",
        "print(\"Target Language; index to word mapping\")\n",
        "convert(targ_lang, target_tensor_train[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input Language; index to word mapping\n",
            "1 ----> <start>\n",
            "14 ----> he\n",
            "20 ----> was\n",
            "54 ----> very\n",
            "618 ----> patient\n",
            "3 ----> .\n",
            "2 ----> <end>\n",
            "\n",
            "Target Language; index to word mapping\n",
            "1 ----> <start>\n",
            "8 ----> او\n",
            "42 ----> خیلی\n",
            "1028 ----> صبور\n",
            "21 ----> بود\n",
            "3 ----> .\n",
            "2 ----> <end>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hrm8Dx7APSXb"
      },
      "source": [
        "Create a tf.data dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o3mHw6GPPA-m"
      },
      "source": [
        "BUFFER_SIZE = len(input_tensor_train)\n",
        "BATCH_SIZE = 64\n",
        "steps_per_epoch = len(input_tensor_train)//BATCH_SIZE\n",
        "embedding_dim = 256\n",
        "units = 1024\n",
        "vocab_inp_size = len(inp_lang.word_index)+1\n",
        "vocab_tar_size = len(targ_lang.word_index)+1\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train)).shuffle(BUFFER_SIZE)\n",
        "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xkEWhsjkPZeL",
        "outputId": "1be8d713-afcc-4324-e623-48b60503aa2f"
      },
      "source": [
        "example_input_batch, example_target_batch = next(iter(dataset))\n",
        "example_input_batch.shape, example_target_batch.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(TensorShape([64, 37]), TensorShape([64, 34]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 208
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "raAdarYiT-eO"
      },
      "source": [
        "This Implementation uses Bahdanau attention for the encoder. Let's decide on notation before writing the simplified form:\n",
        "\n",
        "FC = Fully connected (dense) layer\n",
        "\n",
        "EO = Encoder output\n",
        "\n",
        "H = hidden state\n",
        "\n",
        "X = input to the decoder\n",
        "\n",
        "And the pseudo-code:\n",
        "\n",
        "\n",
        "* score = FC(tanh(FC(EO) + FC(H)))\n",
        "\n",
        "* attention weights = softmax(score, axis = 1). Softmax by default is applied on the last axis but here we want to apply it on the 1st axis, since the shape of score is (batch_size, max_length, hidden_size). Max_length is the length of our input. Since we are trying to assign a weight to each input, softmax should be applied on that axis.\n",
        "\n",
        "* context vector = sum(attention weights * EO, axis = 1). Same reason as above for choosing axis as 1.\n",
        "\n",
        "* embedding output = The input to the decoder X is passed through an embedding layer.\n",
        "\n",
        "* merged vector = concat(embedding output, context vector)\n",
        "This merged vector is then given to the GRU\n",
        "The shapes of all the vectors at each step have been specified in the comments in the code:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HeN82J7iUrAA"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}